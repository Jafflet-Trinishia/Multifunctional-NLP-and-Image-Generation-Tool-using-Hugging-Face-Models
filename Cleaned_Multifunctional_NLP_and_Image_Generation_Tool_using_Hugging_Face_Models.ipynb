{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "0c335d5b605e4bf5bd9e6fb757354675",
      "867e222d1d0d4020b98fd0d4bbbfce9f",
      "effb00b4134c44a5810f5a7c3957be64",
      "ac919747a1674b16a9a3644abf191970",
      "cadac4376ebf417b84e22e1ae66af11f",
      "d6e2bc2b420b4f409072e970479c12d9",
      "45f358bd311740a48c91c1843a5737d9",
      "1d03bf554ae947069426b5b232725e53",
      "814e56d51fd2403196220d87e2bb8889",
      "48f98bce2d4544899bdc3ffcfd24fa03",
      "aa37aa2ef4994152b44b3b572a3adf86",
      "e9cc2cc85ccf4068adee2cb6781180fc",
      "3557b232316641708cb6c7efb037aed1",
      "7d197af4152c488bbe0335d61bc2e6ac",
      "9319ed1680d1485dbad88b4d366d18e6",
      "07334e4e0023415cab8d40ad574c8db6",
      "f0760903da794701b59cf3675dbcd04b",
      "063f51b6e77a47068f41215802406c0b",
      "dfcd3f039c27427c8da3d858e684d98d",
      "fe247eefd37e4f9faa70b404bee341f8",
      "f4007a722a404e40b23c4ed1d7e10178",
      "6e7dda2bd44042979bacec84f9f1598b",
      "e73fcf183a714b5ba1171a62e31aa406",
      "aaf2086c6d9240b38b6ebadb8ecc0f3f",
      "d5439a3428744573bfdc392f84d5c56c",
      "eeebbae79f10458cb7527d2b34046eff",
      "6a231fbb41794d7d8d8dc2897e1722d5",
      "20720d72620949e0998762c9a0f0b4c9",
      "4c65312b97b1445aa65356f727d48925",
      "0f9f8986f56242eaa3a38f9cce22a0ee",
      "d7ae652ad4b44129ab04d0e9d9d172ba",
      "538c3341bbd749fe9991f7cf536b04e8",
      "95f5ddf7d9b04ea2880f1cc4afa0f8c8",
      "7c2832904f234f96a513940a5b8cfee5",
      "1d619bcac34f4530aa1ed6146b07105f",
      "91e302b9055a45fa9069e454526a2640",
      "22f93bf63c594566af2000bcb2e40113",
      "285a2fe8fae84f9d810bc51bae461f0f",
      "16d9c1cfa678410ead076b7645103c6f",
      "8b58b4e005d647c0a7fa833b3c6b5478",
      "0cdc5d89c0724b01a7dbe8892e332b40",
      "7cad1225885f400486ca805e6e8337a8",
      "c07081c91b3f40908e419c8d10bcb6c3",
      "f8f2e0fafc0a4d38a97184ac17a1e6e9",
      "544248769ff14fe5910be702a157fdcd",
      "ece3577031994b20ac853b6c92b6633e",
      "9fa7853f86e74983901a2c6434914117",
      "bf6fe74ffb314947a5612841d3e5867d",
      "0c647696113041d6a309580364349535",
      "60830c6933d541ceae6a7c5ec5136bc9",
      "42be08d91212491da8daff54a4e0fa19",
      "3b9883b9752944e6a39a9ad3bf09d993",
      "acfed662e0cc4a76a6d2a4d5a9341f29",
      "6f8005c88b774871838f86c30fa3a201",
      "480fa3880b464685894d597c4db101cf",
      "0c9dc5f72b554078b8136a9015e72fd7",
      "5db887da958a49a988d2abe09181dbf5",
      "a540381fd14a4413a22a91a23225a9c9",
      "2705576ff10e455bbd6fb62d9809861f",
      "f01cb0919599468abc0a7181d4df3c9d",
      "ecc1865fa0e74cdebf29105c8b50150a",
      "2d946089b3b94a9b8f9782b6e5d986c7",
      "a88479a9338649dc9537cf9add6736de",
      "b47cf123dbb4462e83e9c7e3ec205d36",
      "1140967e66c447e295283470da88922e",
      "e08b2a510beb4446b3723569401c56a1",
      "9e54ce863a2246368c662d3d493a337f",
      "6974de9441074983a0d9d84e70b45e41",
      "c234daceafa04aaeb2215a2116a1fac9",
      "8fee46350f354e2e94b194db68351849",
      "65cc414391e343628e60950a9ad1a625",
      "8c7dd84cc8a346e5a4ea91ad4a2fd789",
      "caf165208769490d8a5e19bb299efaa5",
      "06414fbc05364a0aa055a6c05e767919",
      "7fd11a2e84914f46a0f9e02433b6d1ad",
      "cad1ea846fd54a6faeef2bcfd2fbbcca",
      "7b116636a7994b9b97ae927fedde61ed",
      "d3f2df6744ce4a01b44feb0e3582eeeb",
      "6e4718cf3dcc40e6874713c873ded16a",
      "2609dcc509a6463b98a342caf1404079",
      "b39f6844eb924b3984ef7518a14b7eae",
      "b522730120574f1eab1eee2b6053a10e",
      "2213922135da46a1a415fa7c0e43f299",
      "e52f95eecedf4ddca6832cae11573b09",
      "33bc9c93d4404e499e0e18f320fcd137",
      "c8897f0f39a34b18a3d90788ecd7817e",
      "d2c445a0afdc4d83ba1de4cefb38bb9c",
      "91119bcfd2cc4e2590ed1c4370f98af0",
      "fbfa532fe70b47c293600009ee445b1a",
      "5095e210e34e4fb9aad8eb109beececd",
      "a312553c5c394d7b8777d68cc3fdf3c2",
      "8b3e8f61bfbc4e6fb556255c0de409d1",
      "2e42d4d672b74510a45c441b8f59dff8",
      "99db2281a3fc43969a29afca781f8260",
      "e26b4252ca0849aa83d7f7ea83d60961",
      "ad0ebecc667f4c7088f83acc64ab523b",
      "1b946ab6d9914be0b6285a6b4efcfd28",
      "a004ec80440e43b68aa1e246356d11bc",
      "11be198fc58947e197753b15a6a698ac",
      "823af3f723e3478192f7ebadf90529b3",
      "900c0cc8e6f74e1daa614b19a7948f94",
      "e4c78bd3c87c42b8abf552831fb8c6c5",
      "cb1e9ae060cd4d7c9d28947d7798e2c6",
      "a70d57efd8cc4a75a2388e80254a417c",
      "17871a4bf4c045f38cfd4d750186b97e",
      "5dbaab691c75471d8994f5cb5077803e",
      "f16b69e4f44f43949cb64f7a2258c307",
      "c6553847db6c478db632a47a160e4078",
      "f45b430688434562a65b37bf808ab0ef",
      "8499c7d0fd574a1ca42921a757c10a0e",
      "dedbf9bea9204edba0155686e4634989",
      "6742d75405f84acc8049a737ec1c9f6d",
      "b4185cb2d54744e9af762f912698e993",
      "335b7e12adda45848e029ce03422f0e3",
      "b439b94a544c4e14ae63803ebcbdb413",
      "c5fec1b9188049cf9ae6331e292ed829",
      "fa56bec1a937437381eb5bfa3a27cf85",
      "49537d86deeb4c63a069c061fc22e6ad",
      "5fc71cd0fc91417891aa5924035dffe5",
      "ba32b3cedf4b4c198efdb3acd0a786ee",
      "5401b01f3e24425fb3eb9016558cf875",
      "95a5d454d96f48d48fe3885ea1c6bd7c",
      "d8c534c9feb14ebbad603eb6915dea9c",
      "0e559262ee8145e29b575a8e453383ed",
      "2385351c030b4d11ae8ebe6b317868ee",
      "04767f6bd7e449f6850909fcb034ebd5",
      "9e9c5fb3e5984952b00144b1535c41e5",
      "7b93510d4f3d42b7afe636ab8c860da5",
      "e4811cac82a748e88d7cb115060206dd",
      "d99d03f8c6cc4157a20a81d92ca53583",
      "0606695237cd444196ea44f39176c50a",
      "dfed763373974bd1971644e4647146e6",
      "4f039712fc6643a38ad5ff393bc0a9b8",
      "343466cbf4d74881ae139acca58e853c",
      "0ce2e9a3792c4a3b866ae4b9410bb97f",
      "5127247bbcd44cf89c1e78d98a41d0dd",
      "33f4b37434764f9f8ce4ce4aab0c5f6c",
      "ddccb657db0443fc962c1265d3376d88",
      "eb22619de22045aeb97e81fa7ed633c8",
      "d189d62f0d914817990463cfeb0d5472",
      "f0ff0e3d2fbc4782a793c14bdeec8f7a",
      "bfd79ba386374f1a8d4cfcb955619ce5",
      "737d11d97f4841378380b4e1d2c68ab3",
      "b9e4ad07d5a94e29a6765f18b799f584",
      "f8ccb2756d324531ba0df21c334d62a2",
      "668d733e74b34b5585450f48f62d0e86",
      "e06e890ecf2f4f4e9b7300c39c2c7ad6",
      "b51a3ab43f4f40939a2961eb37061efe",
      "b9bb4ca9114c475d9bdff0166ee4c67e",
      "0973cf1ad6f24817b584c310e5d0ecf1",
      "c87194b71b09490283f8ebc522822ccb",
      "763a678fd84e45d2a4a660646a4304aa",
      "21b5e5bf73a6414584153d8063814607",
      "9ca32f20cc5a4978812ad086875d4a7d",
      "7c23fe3cf71240c3ad8c6032873126bd",
      "7a0b861045b944c487ffb74f0d2cb2df",
      "a68ac62adb334f3fa13588bf2c0f84fa",
      "f5176e61f12548eda6ebefe840bfa22f",
      "d821aefd8367420f9b984c3c24fa86a3",
      "9861705e587b428382ed152418611beb",
      "2cfaddd4d2834d02822d4c8b522665c1",
      "6cb55453954b42e592a5be9c8fc4f6e4",
      "cd894b7436424e518f6c096561320b0c",
      "9ddf4c7e545b408193de44c4a6570527",
      "4120bca859b0423492e895226a7c3ca0",
      "a0b97a0af03e4ddcb354446583c664d5",
      "d3c5d7428d714d9cb40f576fba317f2a",
      "29f0d6d6240544b6959c5ec0e538bcc8",
      "480d07afbaf845168043a1e68fd53205",
      "4b4fd05da7b443e7acac456754c3cb66",
      "807fa6442ee047c4be3b7a8f32d9b8c0",
      "0fb4da2d7116446b83596150450e4383",
      "7b79d5a03c5648d4adc833a2f36e7642",
      "a1b515f3ebb840b6ac2a6580aa36417c",
      "2e6e6f13195740aaa21ff392bf5e0c43",
      "03964774f50741efae5693ba47bf6034",
      "71bc2007eac64bc1881e8661fa90db80",
      "d0dbb9fb38b9416cb6a0428bb278eb62",
      "1f3a3c37a12d430db74974a0b5365c25",
      "2bf3121e12564a6e98bb54e90fae7034",
      "90fbe7d78fb6459d81dc4b007986442b",
      "4e8b08b174c240358f386f1618d854fa",
      "d5c318c150cf46a1a0db5805cefa12f0",
      "9deb263baeb24c60b2557e6aec56bb68",
      "891274fe76e14450b0b62276594b261b",
      "1d016419db04402bbf5630f30367759a",
      "944548f480eb41e9bbd7b6df6936abbd",
      "1c76cf80637a49f9a7591a2efb30d6bc",
      "1c6ab2ab591846cbb68e6af6361b0ec6",
      "ada2f8f1452e4f5d9a8ed18127468962",
      "6c4bca41a43d491c8529e0dc0b863b69",
      "1968595883a2442d86ed76c43ce0d353",
      "ea4c9acc86ee471fb53f9b472288be4c",
      "29625313931c4492a89f57bd16ed17a6",
      "ae36cbcc0eb4498a93d314c87d0b4626",
      "39975572c22f44369a86376cd2c202b5",
      "8335ece18bd249eaaa1467dfd2da4e6a",
      "fa29893d04794dd8b1fb82a7b3b04297",
      "7847f22f6aa54091bef4f5b8c67f1d05",
      "25354024881e45d282a7535102cbc276",
      "fd6d0f6ae72941af960098964a73d1cf",
      "bc2d18d1b87842c692e2820ae06f47ae",
      "a9a35cbe773c46f883d7ba9d6307a065",
      "e1cbfca712f14eeb9e91d5145ecdb817",
      "c70f78c1ce194530bda38ad6cfad59bb",
      "a5b4f133da00456fb51ec554d6a9beac",
      "326b54e1f4254b769ef6fe344b3ef667",
      "49de69fa07624ea1b92d2e9c691dc3fc",
      "ffd67bbf779e4a22a1436f30c1b94325"
     ]
    },
    "id": "IkKFp5saleHL",
    "outputId": "4a080fd8-a32d-4726-83f1-5938469831f7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
      "* Running on public URL: https://b0e6dbfff8daac44f9.gradio.live\n",
      "\n",
      "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c335d5b605e4bf5bd9e6fb757354675",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.42G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9cc2cc85ccf4068adee2cb6781180fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model_index.json:   0%|          | 0.00/541 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e73fcf183a714b5ba1171a62e31aa406",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 15 files:   0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c2832904f234f96a513940a5b8cfee5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/617 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "544248769ff14fe5910be702a157fdcd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/342 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c9dc5f72b554078b8136a9015e72fd7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e54ce863a2246368c662d3d493a337f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3f2df6744ce4a01b44feb0e3582eeeb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.22G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbfa532fe70b47c293600009ee445b1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "scheduler_config.json:   0%|          | 0.00/308 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "823af3f723e3478192f7ebadf90529b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/472 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dedbf9bea9204edba0155686e4634989",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/492M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95a5d454d96f48d48fe3885ea1c6bd7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/743 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f039712fc6643a38ad5ff393bc0a9b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9e4ad07d5a94e29a6765f18b799f584",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/547 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c23fe3cf71240c3ad8c6032873126bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "diffusion_pytorch_model.safetensors:   0%|          | 0.00/3.44G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0b97a0af03e4ddcb354446583c664d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "diffusion_pytorch_model.safetensors:   0%|          | 0.00/335M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71bc2007eac64bc1881e8661fa90db80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/806 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c76cf80637a49f9a7591a2efb30d6bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7847f22f6aa54091bef4f5b8c67f1d05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# âœ… Install all required libraries\n",
    "!pip install transformers gradio diffusers torch accelerate evaluate rouge_score bert_score matplotlib sentence-transformers --quiet\n",
    "\n",
    "# -------- Library Imports --------\n",
    "import re\n",
    "import io\n",
    "import gc\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import gradio as gr\n",
    "from transformers import (\n",
    "    pipeline,\n",
    "    GPT2LMHeadModel,\n",
    "    GPT2TokenizerFast,\n",
    "    BlenderbotTokenizer,\n",
    "    BlenderbotForConditionalGeneration,\n",
    "    CLIPProcessor,\n",
    "    CLIPModel\n",
    ")\n",
    "\n",
    "from diffusers import StableDiffusionPipeline\n",
    "import evaluate\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "# -------- Device Detection --------\n",
    "if torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "elif torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "\n",
    "device_id = 0 if device != \"cpu\" else -1\n",
    "pipe = None  # For image generation\n",
    "\n",
    "# -------- Load Evaluation Metrics --------\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "bertscore = evaluate.load(\"bertscore\")\n",
    "meteor = evaluate.load(\"meteor\")\n",
    "embed_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "fluency_model = GPT2LMHeadModel.from_pretrained(\"gpt2\").to(device)\n",
    "fluency_tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")\n",
    "clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(device)\n",
    "clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "chat_model = BlenderbotForConditionalGeneration.from_pretrained(\"facebook/blenderbot-400M-distill\").to(device)\n",
    "\n",
    "def compute_clip_score(image: Image, prompt: str):\n",
    "    try:\n",
    "        inputs = clip_processor(text=[prompt], images=image, return_tensors=\"pt\", padding=True).to(device)\n",
    "        with torch.no_grad():\n",
    "            outputs = clip_model(**inputs)\n",
    "            score = outputs.logits_per_image.softmax(dim=1).item()\n",
    "        return score\n",
    "    except Exception as e:\n",
    "        return 0.0\n",
    "\n",
    "def plot_clip_score(score):\n",
    "    fig, ax = plt.subplots(figsize=(4, 2.5))\n",
    "    ax.bar([\"CLIPScore\"], [score], color='green')\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.set_ylabel(\"Score\")\n",
    "    ax.set_title(\"Image-Text Similarity (CLIPScore)\")\n",
    "    ax.grid(axis='y')\n",
    "    buf = io.BytesIO()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(buf, format='png')\n",
    "    buf.seek(0)\n",
    "    return Image.open(buf)\n",
    "\n",
    "def compute_perplexity(text):\n",
    "    enc = fluency_tokenizer(text, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        output = fluency_model(**enc, labels=enc.input_ids)\n",
    "    loss = output.loss\n",
    "    return np.exp(loss.item())\n",
    "\n",
    "def evaluate_chitchat_similarity(response, reference):\n",
    "    if not reference.strip():\n",
    "        return \"\", None\n",
    "\n",
    "    # Semantic similarity\n",
    "    emb1 = embed_model.encode(response, convert_to_tensor=True)\n",
    "    emb2 = embed_model.encode(reference, convert_to_tensor=True)\n",
    "    similarity = util.cos_sim(emb1, emb2).item()\n",
    "\n",
    "    # Fluency via perplexity\n",
    "    fluency = compute_perplexity(response)\n",
    "    fluency_score = max(0.0, 1 - np.log(fluency) / 10)  # Normalize to 0â€“1\n",
    "\n",
    "    # Length score\n",
    "    token_len = len(response.split())\n",
    "    length_score = min(1.0, token_len / 30)  # Ideal = 30 tokens\n",
    "\n",
    "    # Plot chart\n",
    "    fig, ax = plt.subplots(figsize=(6, 2.5))\n",
    "    labels = [\"Semantic\", \"Fluency\", \"Length\"]\n",
    "    values = [similarity, fluency_score, length_score]\n",
    "    ax.bar(labels, values, color='teal')\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.set_title(\"Chitchat Evaluation Metrics\")\n",
    "    ax.set_ylabel(\"Score\")\n",
    "    ax.grid(axis='y')\n",
    "\n",
    "    buf = io.BytesIO()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(buf, format='png')\n",
    "    buf.seek(0)\n",
    "    image = Image.open(buf)\n",
    "\n",
    "    return f\"ðŸ§  Semantic: {similarity:.2f} | Fluency: {fluency_score:.2f} | Length: {length_score:.2f}\", image\n",
    "\n",
    "def compute_metrics(prediction, reference):\n",
    "    try:\n",
    "        rouge_score = rouge.compute(predictions=[prediction], references=[reference])[\"rougeL\"]\n",
    "        bert_score = bertscore.compute(predictions=[prediction], references=[reference], lang=\"en\")[\"f1\"][0]\n",
    "        meteor_score = meteor.compute(predictions=[prediction], references=[reference])[\"meteor\"]\n",
    "\n",
    "        # Generate chart\n",
    "        fig, ax = plt.subplots(figsize=(6, 3))\n",
    "        labels = [\"ROUGE-L\", \"BERTScore-F1\", \"METEOR\"]\n",
    "        values = [rouge_score, bert_score, meteor_score]\n",
    "        ax.plot(labels, values, marker='o', linestyle='-', color='blue')\n",
    "        ax.set_ylim(0, 1)\n",
    "        ax.set_title(\"Evaluation Metrics\")\n",
    "        ax.set_ylabel(\"Score\")\n",
    "        ax.grid(True)\n",
    "\n",
    "        buf = io.BytesIO()\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(buf, format='png')\n",
    "        buf.seek(0)\n",
    "        image = Image.open(buf)\n",
    "        return f\"ðŸ“Š ROUGE-L: {rouge_score:.4f} | BERTScore-F1: {bert_score:.4f} | METEOR: {meteor_score:.4f}\", image\n",
    "    except Exception as e:\n",
    "        return f\"Evaluation error: {e}\", None\n",
    "\n",
    "# -------- Load All NLP Pipelines --------\n",
    "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\", device=device_id)\n",
    "sentiment_analyzer = pipeline(\"sentiment-analysis\", model=\"distilbert/distilbert-base-uncased-finetuned-sst-2-english\", device=device_id)\n",
    "qa_pipeline = pipeline(\"question-answering\", model=\"distilbert/distilbert-base-cased-distilled-squad\", device=device_id)\n",
    "next_word_generator = pipeline(\"text-generation\", model=\"distilgpt2\", device=device_id)\n",
    "story_generator = pipeline(\"text-generation\", model=\"aspis/gpt2-genre-story-generation\", device=device_id)\n",
    "chat_tokenizer = BlenderbotTokenizer.from_pretrained(\"facebook/blenderbot-400M-distill\")\n",
    "\n",
    "# -------- Memory Cleanup --------\n",
    "def clear_memory():\n",
    "    gc.collect()\n",
    "    if device == \"cuda\":\n",
    "        torch.cuda.empty_cache()\n",
    "    elif device == \"mps\":\n",
    "        from torch.mps import empty_cache\n",
    "        empty_cache()\n",
    "\n",
    "# -------- Utility Functions --------\n",
    "def clean_incomplete_output(text):\n",
    "    text = text.replace(\"<n>\", \" \")\n",
    "    sentences = re.split(r'(?<=[.!?]) +', text.strip())\n",
    "    cleaned = [s for s in sentences if s and s[0].isupper() and s[-1] in \".!?\" and len(s.split()) >= 4]\n",
    "    return ' '.join(cleaned) or text\n",
    "\n",
    "def clean_at_least_two_sentences(text):\n",
    "    sentences = re.split(r'(?<=[.!?]) +', text.strip())\n",
    "    cleaned = [s for s in sentences if s and s[0].isupper() and s[-1] in \".!?\" and len(s.split()) >= 4]\n",
    "    return ' '.join(cleaned[:2]) if len(cleaned) >= 2 else ' '.join(cleaned) if cleaned else text\n",
    "\n",
    "# -------- NLP Task Functions --------\n",
    "def summarize(text, reference=None):\n",
    "    if len(text.split()) < 10:\n",
    "        return text, None, \"\"\n",
    "\n",
    "    max_len = min(60, max(50, int(len(text.split()) * 0.7)))\n",
    "    min_len = max(20, int(max_len * 0.5))\n",
    "\n",
    "    try:\n",
    "        result = summarizer(\n",
    "            text,\n",
    "            max_length=max_len,\n",
    "            min_length=min_len,\n",
    "            do_sample=True,\n",
    "            top_k=50,\n",
    "            top_p=0.92,\n",
    "            temperature=0.8,\n",
    "            truncation=True\n",
    "        )\n",
    "        clear_memory()\n",
    "        raw_summary = result[0]['summary_text']\n",
    "        cleaned = clean_incomplete_output(raw_summary)\n",
    "        metrics, chart = compute_metrics(cleaned, reference) if reference else (\"\", None)\n",
    "        return cleaned, chart, metrics\n",
    "    except Exception as e:\n",
    "        return f\"Summarization error: {e}\", None, \"\"\n",
    "\n",
    "def predict_next_word(prompt):\n",
    "    if not prompt.strip():\n",
    "        return \"Please enter a valid prompt.\"\n",
    "    trimmed = ' '.join(prompt.split()[:20])\n",
    "    try:\n",
    "        result = next_word_generator(\n",
    "            trimmed,\n",
    "            max_new_tokens=15,\n",
    "            do_sample=True,\n",
    "            top_k=50,\n",
    "            top_p=0.95,\n",
    "            temperature=0.9\n",
    "        )\n",
    "        return clean_at_least_two_sentences(result[0]['generated_text'])\n",
    "    except Exception as e:\n",
    "        return f\"Prediction error: {e}\"\n",
    "\n",
    "def predict_story(prompt):\n",
    "    if not prompt.strip():\n",
    "        return \"Please enter a story prompt.\"\n",
    "    seed = \"Once upon a time, \" + prompt.strip().capitalize()\n",
    "    try:\n",
    "        result = story_generator(\n",
    "            seed,\n",
    "            max_new_tokens=80,\n",
    "            do_sample=True,\n",
    "            top_k=50,\n",
    "            top_p=0.95,\n",
    "            temperature=0.9\n",
    "        )\n",
    "        return clean_at_least_two_sentences(result[0]['generated_text'])\n",
    "    except Exception as e:\n",
    "        return f\"Story generation error: {e}\"\n",
    "\n",
    "def chat_response(text):\n",
    "    inputs = chat_tokenizer([text], return_tensors=\"pt\").to(device)\n",
    "    reply_ids = chat_model.generate(**inputs)\n",
    "    reply = chat_tokenizer.batch_decode(reply_ids, skip_special_tokens=True)[0]\n",
    "    clear_memory()\n",
    "    return reply\n",
    "\n",
    "def analyze_sentiment(text):\n",
    "    result = sentiment_analyzer(text)[0]\n",
    "    clear_memory()\n",
    "    return f\"{result['label']} ({result['score']:.2f})\"\n",
    "\n",
    "def answer_question(question, context):\n",
    "    result = qa_pipeline(question=question, context=context)['answer']\n",
    "    clear_memory()\n",
    "    return result\n",
    "\n",
    "def generate_image(prompt):\n",
    "    global pipe\n",
    "    if pipe is None:\n",
    "        pipe = StableDiffusionPipeline.from_pretrained(\"runwayml/stable-diffusion-v1-5\").to(device)\n",
    "    image = pipe(prompt).images[0]\n",
    "    clear_memory()\n",
    "    return image\n",
    "\n",
    "# -------- Gradio UI --------\n",
    "def handle_tasks(task_type, input_text_val, reference_text_val):\n",
    "    try:\n",
    "        if task_type == \"Text Summarization\":\n",
    "            summary, chart, metrics = summarize(input_text_val, reference_text_val)\n",
    "            return summary, gr.update(visible=False), gr.update(value=chart, visible=True) if chart else gr.update(visible=False), metrics\n",
    "\n",
    "        elif task_type == \"Image Generation\":\n",
    "            image = generate_image(input_text_val)\n",
    "            clip_score = compute_clip_score(image, input_text_val)\n",
    "            chart = plot_clip_score(clip_score)\n",
    "            return \"\", gr.update(value=image, visible=True), gr.update(value=chart, visible=True), f\"CLIPScore: {clip_score:.2f}\"\n",
    "\n",
    "        elif task_type == \"Next Word Prediction\":\n",
    "            output = predict_next_word(input_text_val)\n",
    "            if reference_text_val.strip():\n",
    "                metrics, chart = compute_metrics(output, reference_text_val)\n",
    "                return output, gr.update(visible=False), gr.update(value=chart, visible=True), metrics\n",
    "            else:\n",
    "                return output, gr.update(visible=False), gr.update(visible=False), \"\"\n",
    "\n",
    "        elif task_type == \"Story Prediction\":\n",
    "            output = predict_story(input_text_val)\n",
    "            if reference_text_val.strip():\n",
    "                metrics, chart = compute_metrics(output, reference_text_val)\n",
    "                return output, gr.update(visible=False), gr.update(value=chart, visible=True), metrics\n",
    "            else:\n",
    "                return output, gr.update(visible=False), gr.update(visible=False), \"\"\n",
    "\n",
    "        elif task_type == \"Chatbot\":\n",
    "            output = chat_response(input_text_val)\n",
    "            if reference_text_val.strip():\n",
    "                metrics, chart = evaluate_chitchat_similarity(output, reference_text_val)\n",
    "                return output, gr.update(visible=False), gr.update(value=chart, visible=True), metrics\n",
    "            else:\n",
    "                return output, gr.update(visible=False), gr.update(visible=False), \"\"\n",
    "\n",
    "        elif task_type == \"Sentiment Analysis\":\n",
    "            output = analyze_sentiment(input_text_val)\n",
    "            if reference_text_val.strip():\n",
    "                metrics, chart = compute_metrics(output, reference_text_val)\n",
    "                return output, gr.update(visible=False), gr.update(value=chart, visible=True), metrics\n",
    "            else:\n",
    "                return output, gr.update(visible=False), gr.update(visible=False), \"\"\n",
    "\n",
    "        elif task_type == \"Question Answering\":\n",
    "            output = answer_question(input_text_val, reference_text_val)\n",
    "            if reference_text_val.strip():\n",
    "                metrics, chart = compute_metrics(output, reference_text_val)\n",
    "                return output, gr.update(visible=False), gr.update(value=chart, visible=True), metrics\n",
    "            else:\n",
    "                return output, gr.update(visible=False), gr.update(visible=False), \"\"\n",
    "\n",
    "        else:\n",
    "            return \"âŒ Unsupported task.\", gr.update(visible=False), gr.update(visible=False), \"\"\n",
    "\n",
    "    except Exception as e:\n",
    "        return f\"{task_type} error: {e}\", gr.update(visible=False), gr.update(visible=False), \"\"\n",
    "\n",
    "def clear_all():\n",
    "    return \"\", \"\", \"\", gr.update(visible=False), gr.update(visible=False), \"\"\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    gr.Markdown(\"# ðŸ¤– Multifunctional NLP & Image Generation Tool\")\n",
    "\n",
    "    task = gr.Dropdown(\n",
    "        choices=[\n",
    "            \"Text Summarization\",\n",
    "            \"Next Word Prediction\",\n",
    "            \"Story Prediction\",\n",
    "            \"Chatbot\",\n",
    "            \"Sentiment Analysis\",\n",
    "            \"Question Answering\",\n",
    "            \"Image Generation\"\n",
    "        ],\n",
    "        label=\"Choose Task\",\n",
    "        interactive=True\n",
    "    )\n",
    "\n",
    "    input_text = gr.Textbox(label=\"Enter Input\")\n",
    "    reference_input = gr.Textbox(label=\"Reference Output (Optional)\")\n",
    "    output_text = gr.Textbox(label=\"Output (Text)\", lines=4)\n",
    "    generated_image = gr.Image(label=\"Generated Image\", visible=False)\n",
    "    graph_image = gr.Image(label=\"Evaluation Chart\", visible=False)\n",
    "    metrics_output = gr.Textbox(label=\"Evaluation Metrics\", interactive=False)\n",
    "\n",
    "    clear_button = gr.Button(\"Clear All\")\n",
    "    generate_button = gr.Button(\"Generate Output\")\n",
    "\n",
    "    gr.Markdown(\"### ðŸ“Œ Notes:\\n- Reference is optional but used for evaluation chart.\")\n",
    "\n",
    "    generate_button.click(\n",
    "        fn=handle_tasks,\n",
    "        inputs=[task, input_text, reference_input],\n",
    "        outputs=[output_text, generated_image, graph_image, metrics_output]\n",
    "    )\n",
    "\n",
    "    clear_button.click(\n",
    "        fn=clear_all,\n",
    "        inputs=[],\n",
    "        outputs=[input_text, reference_input, output_text, generated_image, graph_image, metrics_output]\n",
    "    )\n",
    "\n",
    "demo.launch(share=True, inline=False, debug=True)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
